# MobiAttn

Mobile Attention(MobiAttn) is a head-only library designed for transformer-based models. Currently in its early stages, the library aims to explore the application and performance of various attention algorithms on mobile devices.

## Key Features

1. [FlashAttention](https://arxiv.org/abs/2307.08691)
    - Causal Mask
    - ​​Multi-Head & Grouped-Query Attention​​
    - Arm Neon SIMD optimizations with OpenMP parallelization for mobile CPUs

## How to use?

MobiAttn is a head-only library.

```
git clone https://github.com/chenghuaWang/MobiAttn
```

## What's Next?
